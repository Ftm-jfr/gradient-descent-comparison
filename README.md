# Gradient Descent Comparison

This project provides a side-by-side comparison of three popular gradient descent algorithms, applied in two separate settings:

- **Batch Gradient Descent**
- **Stochastic Gradient Descent (SGD)**
- **Mini-Batch Gradient Descent**

Each method is evaluated using two different loss functions:
- **Mean Squared Error (MSE)** â€“ for standard regression
- **Rosenbrock Function** â€“ to demonstrate optimization behavior in a non-convex landscape

### ðŸ“Š Metrics Compared:
- Convergence speed (execution time)
- Number of weight updates
- Loss reduction over epochs

The results include:
- Loss plots for each optimizer
- Optimization path visualization on 2D contour plots (w vs. b)
